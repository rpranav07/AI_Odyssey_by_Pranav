# Pillar 3: Machine Learning Algorithms and Design

## Machine Learning Algorithms and Techniques

| ✅ | Topic                           | Subtopic                              | Best Resources                                                                                       | Projects/Assignments                          | Evaluation Criteria                      |
|----|---------------------------------|---------------------------------------|------------------------------------------------------------------------------------------------------|-----------------------------------------------|------------------------------------------|
| ⬜ | **Data Preprocessing**           | Handling missing values, normalization, scaling | [StatQuest - Data Preprocessing](https://www.youtube.com/user/joshstarmer), [Kaggle Tutorials](https://www.kaggle.com/learn/data-cleaning) | Preprocess a dataset with missing values      | Correctness, clarity                    |
| ⬜ | **Feature Engineering**          | Encoding categorical data, feature scaling, feature selection | [Hands-On Machine Learning](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/), [Kaggle Feature Engineering](https://www.kaggle.com/learn/feature-engineering) | Implement one-hot encoding and PCA on a dataset | Accuracy improvement, explained variance |
| ⬜ | **Feature Selection**            | Recursive feature elimination (RFE), variance threshold | [StatQuest - Feature Selection](https://www.youtube.com/user/joshstarmer), [Scikit-learn Docs](https://scikit-learn.org/stable/) | Perform feature selection on a dataset        | Improved model performance              |
| ⬜ | **Exploratory Data Analysis (EDA)** | Descriptive statistics, visualizations | [Data School - EDA](https://www.dataschool.io/), [Seaborn Docs](https://seaborn.pydata.org/)         | Analyze and visualize a dataset               | Insightfulness, plot clarity            |
| ⬜ | **Regression Algorithms**        | Linear regression, Ridge, Lasso, Polynomial regression | [Hands-On Machine Learning](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/), [StatQuest](https://www.youtube.com/user/joshstarmer) | Predict housing prices using regression       | R2 score, MSE                           |
| ⬜ | **Classification Algorithms**    | Logistic regression, Naive Bayes, KNN | [Scikit-learn Docs](https://scikit-learn.org/stable/), [StatQuest](https://www.youtube.com/user/joshstarmer) | Build a spam email classifier                 | Accuracy, F1 score                      |
| ⬜ | **Tree-based Models**            | Decision Trees, Random Forest, Gradient Boosting | [StatQuest - Decision Trees](https://www.youtube.com/user/joshstarmer), [XGBoost Docs](https://xgboost.readthedocs.io/en/stable/) | Predict customer churn using Random Forest    | Accuracy, ROC-AUC                       |
| ⬜ | **Support Vector Machines (SVM)** | Linear SVM, kernel SVM                 | [Hands-On Machine Learning](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/) | Classify handwritten digits                   | Accuracy, precision, recall             |
| ⬜ | **Clustering Algorithms**        | K-Means, DBSCAN, Hierarchical clustering | [Scikit-learn Clustering](https://scikit-learn.org/stable/modules/clustering.html), [StatQuest](https://www.youtube.com/user/joshstarmer) | Cluster customers based on purchasing behavior | Silhouette score                        |
| ⬜ | **Dimensionality Reduction**     | PCA, t-SNE, UMAP                       | [Scikit-learn PCA](https://scikit-learn.org/stable/modules/decomposition.html), [Hands-On Machine Learning](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/) | Apply PCA to reduce dimensions of a dataset   | Explained variance ratio                |
| ⬜ | **Ensemble Methods**             | Bagging, boosting, stacking           | [StatQuest - Ensembles](https://www.youtube.com/user/joshstarmer), [XGBoost Docs](https://xgboost.readthedocs.io/en/stable/) | Combine multiple models for better accuracy   | Accuracy, ROC-AUC                       |
| ⬜ | **Hyperparameter Tuning**        | Grid search, Random search, Bayesian optimization | [Scikit-learn Grid Search](https://scikit-learn.org/stable/modules/grid_search.html), [Optuna](https://optuna.org/) | Tune a Random Forest model for optimal performance | Accuracy, efficiency                    |
| ⬜ | **Cross-validation**             | K-fold, stratified K-fold, Leave-one-out | [Scikit-learn Cross-validation](https://scikit-learn.org/stable/modules/cross_validation.html)        | Evaluate a model using K-fold CV              | Consistency of metrics                  |

---

